{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp685oYP-e-X"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3cqbvTCoCS8G",
        "outputId": "aa0406ec-4911-41aa-93b9-86595d0cacf7"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zXN0tKoCV8s"
      },
      "source": [
        "input_embedding = [[\n",
        "  \"Salut\", \"comment\", \"ca\", \"va\", \"?\"\n",
        "]]\n",
        "\n",
        "\n",
        "output_embedding = [[\n",
        "    \"<START>\", \"Hi\", \"how\", \"are\", \"you\", \"?\"\n",
        "]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqufPyGeC0q5",
        "outputId": "ab1aa78d-c7a9-4803-c853-d2da9db04b9d"
      },
      "source": [
        "def get_vocabulary(sequences):\n",
        "\n",
        "  token_to_info = {}\n",
        "\n",
        "  for sequence in sequences:\n",
        "    for word in sequence:\n",
        "      if word not in token_to_info:\n",
        "        token_to_info[word] = len(token_to_info)\n",
        "\n",
        "  return token_to_info\n",
        "\n",
        "input_voc = get_vocabulary(input_embedding)\n",
        "output_voc = get_vocabulary(output_embedding)\n",
        "\n",
        "input_voc[\"<START>\"] = len(input_voc)\n",
        "input_voc[\"<END>\"] = len(input_voc)\n",
        "input_voc[\"<PAD>\"] = len(input_voc)\n",
        "\n",
        "output_voc[\"<END>\"] = len(output_voc)\n",
        "output_voc[\"<PAD>\"] = len(output_voc)\n",
        "\n",
        "print(input_voc)\n",
        "print(output_voc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Salut': 0, 'comment': 1, 'ca': 2, 'va': 3, '?': 4, '<START>': 5, '<END>': 6, '<PAD>': 7}\n",
            "{'<START>': 0, 'Hi': 1, 'how': 2, 'are': 3, 'you': 4, '?': 5, '<END>': 6, '<PAD>': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRKhgcDhEcAW",
        "outputId": "86fd5079-e019-4bca-85c4-83c3d74c699e"
      },
      "source": [
        "def sequences_to_int(sequences, voc):\n",
        "  for sequence in sequences:\n",
        "    print(sequence)\n",
        "    for s, word in enumerate(sequence):\n",
        "      sequence[s] = voc[word]\n",
        "  return np.array(sequences)\n",
        "\n",
        "input_seq = sequences_to_int(input_embedding, input_voc)\n",
        "output_seq = sequences_to_int(output_embedding, output_voc)\n",
        "\n",
        "print(\"input_seq\", input_seq)\n",
        "print(\"output_seq\", output_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Salut', 'comment', 'ca', 'va', '?']\n",
            "['<START>', 'Hi', 'how', 'are', 'you', '?']\n",
            "input_seq [[0 1 2 3 4]]\n",
            "output_seq [[0 1 2 3 4 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CM321hbFy-G",
        "outputId": "ce30a4d4-55c5-4804-c903-691fad8683fa"
      },
      "source": [
        "\n",
        "\n",
        "class EmbeddingLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_token, **kwargs):\n",
        "    self.nb_token = nb_token\n",
        "    super(**kwargs).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.word_embedding = tf.keras.layers.Embedding(\n",
        "        self.nb_token, 256\n",
        "    )\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    embed = self.word_embedding(x)\n",
        "    return embed\n",
        "\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.query_layer = tf.keras.layers.Dense(256)\n",
        "    self.value_layer = tf.keras.layers.Dense(256)\n",
        "    self.key_layer = tf.keras.layers.Dense(256)\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    Q = self.query_layer(x)\n",
        "    K = self.key_layer(x)\n",
        "    V = self.value_layer(x)\n",
        "    QK = tf.matmul(Q, K, transpose_b=True)\n",
        "    QK = QK / tf.math.sqrt(256.)\n",
        "    softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
        "    attention = tf.matmul(softmax_QK, V)\n",
        "    return attention\n",
        "\n",
        "def test():\n",
        "  layer_input = tf.keras.Input(shape=(5))\n",
        "  embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
        "  attention = ScaledDotProductAttention()(embedding)\n",
        "  model = tf.keras.Model(layer_input, attention)\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "m_test = test()\n",
        "out = m_test(input_seq)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_30 (InputLayer)        [(None, 5)]               0         \n",
            "_________________________________________________________________\n",
            "embedding_layer_25 (Embeddin (None, 5, 256)            1280      \n",
            "_________________________________________________________________\n",
            "scaled_dot_product_attention (None, 5, 256)            197376    \n",
            "=================================================================\n",
            "Total params: 198,656\n",
            "Trainable params: 198,656\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(1, 5, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEiTxF7yMsLk",
        "outputId": "6384c680-1f12-4486-fc8f-4f9e0d6b0f9b"
      },
      "source": [
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dim=256, nb_head=8, **kwargs):\n",
        "    self.dim = dim\n",
        "    self.head_dim = 256 // 8\n",
        "    self.nb_head = nb_head\n",
        "    super(**kwargs).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.query_layer = tf.keras.layers.Dense(256)\n",
        "    self.value_layer = tf.keras.layers.Dense(256)\n",
        "    self.key_layer = tf.keras.layers.Dense(256)\n",
        "    self.out_proj = tf.keras.layers.Dense(256)\n",
        "    super().build(input_shape)\n",
        "\n",
        "\n",
        "  def mask_softmax(self, x, mask):\n",
        "    x_expe = tf.math.exp(x)\n",
        "    x_expe_masked = x_expe * mask\n",
        "    x_expe_sum = tf.reduce_sum(x_expe_masked, axis=-1)\n",
        "    x_expe_sum = tf.expand_dims(x_expe_sum, axis=-1)\n",
        "    softmax = x_expe_masked / x_expe_sum\n",
        "    return softmax\n",
        "\n",
        "\n",
        "  def call(self, x, mask=None):\n",
        "\n",
        "    in_query, in_key, in_value = x\n",
        "\n",
        "    Q = self.query_layer(in_query)\n",
        "    K = self.key_layer(in_key)\n",
        "    V = self.value_layer(in_value)\n",
        "\n",
        "    batch_size = tf.shape(Q)[0]\n",
        "    Q_seq_len = tf.shape(Q)[1]\n",
        "    K_seq_len = tf.shape(K)[1]\n",
        "    V_seq_len = tf.shape(V)[1]\n",
        "\n",
        "    Q = tf.reshape(Q, [batch_size, Q_seq_len, self.nb_head, self.head_dim])\n",
        "    K = tf.reshape(K, [batch_size, K_seq_len, self.nb_head, self.head_dim])\n",
        "    V = tf.reshape(V, [batch_size, V_seq_len, self.nb_head, self.head_dim])\n",
        "\n",
        "    Q = tf.transpose(Q, [0, 2, 1, 3])\n",
        "    K = tf.transpose(K, [0, 2, 1, 3])\n",
        "    V = tf.transpose(V, [0, 2, 1, 3])\n",
        "\n",
        "    Q = tf.reshape(Q, [batch_size * self.nb_head, Q_seq_len, self.head_dim])\n",
        "    K = tf.reshape(K, [batch_size * self.nb_head, K_seq_len, self.head_dim])\n",
        "    V = tf.reshape(V, [batch_size * self.nb_head, V_seq_len, self.head_dim])\n",
        "\n",
        "    # Scaled dot product attention\n",
        "    QK = tf.matmul(Q, K, transpose_b=True)\n",
        "    QK = QK / tf.math.sqrt(256.)\n",
        "\n",
        "    # [1, 0, 0, 0, 0, 0]\n",
        "    # [1, 1, 0, 0, 0, 0]\n",
        "    # [1, 1, 1, 0, 0, 0]\n",
        "    # [1, 1, 1, 1, 0, 0]\n",
        "    # [1, 1, 1, 1, 0, 0]\n",
        "\n",
        "    if mask is not None:\n",
        "      QK = QK * mask\n",
        "      #print(\"mask\", mask.shape)\n",
        "      #print(\"QK\", QK.shape)\n",
        "      softmax_QK = self.mask_softmax(QK, mask)\n",
        "    else:\n",
        "      softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
        "\n",
        "    attention = tf.matmul(softmax_QK, V)\n",
        "\n",
        "    attention = tf.reshape(\n",
        "        attention, [batch_size, self.nb_head, Q_seq_len, self.head_dim])\n",
        "\n",
        "    attention = tf.transpose(attention, [0, 2, 1, 3])\n",
        "    # Concat\n",
        "    attention = tf.reshape(\n",
        "        attention, [batch_size, Q_seq_len, self.nb_head*self.head_dim]\n",
        "    )\n",
        "\n",
        "    out_attention = self.out_proj(attention)\n",
        "\n",
        "    return out_attention\n",
        "\n",
        "\n",
        "def test():\n",
        "  layer_input = tf.keras.Input(shape=(6))\n",
        "  embedding = EmbeddingLayer(nb_token=6)(layer_input)\n",
        "\n",
        "  # mask\n",
        "  mask = tf.sequence_mask(tf.range(6) + 1, 6)\n",
        "  mask = tf.cast(mask, tf.float32)\n",
        "  mask = tf.expand_dims(mask, axis=0)\n",
        "\n",
        "  multi_attention = MultiHeadAttention()((embedding, embedding, embedding), mask=mask)\n",
        "\n",
        "  model = tf.keras.Model(layer_input, multi_attention)\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "m_test = test()\n",
        "out = m_test(output_seq)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_95\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_144 (InputLayer)          [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_layer_161 (EmbeddingL (None, 6, 256)       1536        input_144[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_48 (MultiH (None, None, 256)    263168      embedding_layer_161[0][0]        \n",
            "                                                                 embedding_layer_161[0][0]        \n",
            "                                                                 embedding_layer_161[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 264,704\n",
            "Trainable params: 264,704\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "(1, 6, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhx3-nJdLK2P",
        "outputId": "3d4a9730-f3dc-47e6-ef11-db94320f6efa"
      },
      "source": [
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.multi_head_attention = MultiHeadAttention()\n",
        "    self.norm = tf.keras.layers.LayerNormalization()\n",
        "    self.dense_out = tf.keras.layers.Dense(256)\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    attention = self.multi_head_attention((x, x, x))\n",
        "    post_attention = self.norm(attention + x)\n",
        "\n",
        "    x = self.dense_out(post_attention)\n",
        "    enc_output = self.norm(x + post_attention)\n",
        "\n",
        "    return enc_output\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_encoder, **kwargs):\n",
        "    self.nb_encoder = nb_encoder\n",
        "    super(**kwargs).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "\n",
        "    self.encoder_layers = []\n",
        "    for nb in range(self.nb_encoder):\n",
        "      self.encoder_layers.append(\n",
        "          EncoderLayer()\n",
        "      )\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def test():\n",
        "  layer_input = tf.keras.Input(shape=(5))\n",
        "  embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
        "  enc_output = Encoder(nb_encoder=6)(embedding)\n",
        "  model = tf.keras.Model(layer_input, enc_output)\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "m_test = test()\n",
        "out = m_test(input_seq)\n",
        "print(out.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_80\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_92 (InputLayer)        [(None, 5)]               0         \n",
            "_________________________________________________________________\n",
            "embedding_layer_87 (Embeddin (None, 5, 256)            1280      \n",
            "_________________________________________________________________\n",
            "encoder_7 (Encoder)          (None, 5, 256)            1976832   \n",
            "=================================================================\n",
            "Total params: 1,978,112\n",
            "Trainable params: 1,978,112\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(1, 5, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikgHrH4QSI7m",
        "outputId": "355dea15-e6f2-4259-eff5-44ed462e1c82"
      },
      "source": [
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.multi_head_self_attention = MultiHeadAttention()\n",
        "    self.multi_head_enc_attention = MultiHeadAttention()\n",
        "    self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    self.proj_output = tf.keras.layers.Dense(256)\n",
        "\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    enc_output, output_embedding, mask = x\n",
        "\n",
        "    self_attention = self.multi_head_self_attention(\n",
        "        (output_embedding, output_embedding, output_embedding), mask=mask\n",
        "    )\n",
        "    post_self_att = self.norm(output_embedding + self_attention)\n",
        "\n",
        "    enc_attention = self.multi_head_self_attention(\n",
        "        (post_self_att, enc_output, enc_output)\n",
        "    )\n",
        "    post_enc_attention = self.norm(enc_attention + post_self_att)\n",
        "    proj_out = self.proj_output(post_enc_attention)\n",
        "\n",
        "    dec_output = self.norm(proj_out + post_enc_attention)\n",
        "\n",
        "    return dec_output\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_decoder, **kwargs):\n",
        "    self.nb_decoder = nb_decoder\n",
        "    super(**kwargs).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "\n",
        "    self.decoder_layers = []\n",
        "    for nb in range(self.nb_decoder):\n",
        "      self.decoder_layers.append(\n",
        "          DecoderLayer()\n",
        "      )\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    enc_output, output_embedding, mask = x\n",
        "\n",
        "    dec_output = output_embedding\n",
        "    for decoder_layer in self.decoder_layers:\n",
        "      dec_output = decoder_layer((enc_output, dec_output, mask))\n",
        "    return dec_output\n",
        "\n",
        "\n",
        "def get_transformer_model(output_voc):\n",
        "  input_token = tf.keras.Input(shape=(5))\n",
        "  output_token = tf.keras.Input(shape=(6))\n",
        "\n",
        "  # Positional encoding\n",
        "  input_pos_encoding = EmbeddingLayer(nb_token=5)(tf.range(5))\n",
        "  output_pos_encoding = EmbeddingLayer(nb_token=6)(tf.range(6))\n",
        "\n",
        "  # Retrieve embedding\n",
        "  input_embedding = EmbeddingLayer(nb_token=5)(input_token)\n",
        "  output_embedding = EmbeddingLayer(nb_token=6)(output_token)\n",
        "\n",
        "  # Add the positional encoding\n",
        "  input_embedding = input_embedding + input_pos_encoding\n",
        "  output_embedding = output_embedding + output_pos_encoding\n",
        "\n",
        "  # Encoder\n",
        "  enc_output = Encoder(nb_encoder=6)(input_embedding)\n",
        "\n",
        "  # mask + Decoder\n",
        "  mask = tf.sequence_mask(tf.range(6) + 1, 6)\n",
        "  mask = tf.cast(mask, tf.float32)\n",
        "  mask = tf.expand_dims(mask, axis=0)\n",
        "  dec_output = Decoder(nb_decoder=6)((enc_output, output_embedding, mask))\n",
        "\n",
        "\n",
        "  # Predictions\n",
        "  out_pred = tf.keras.layers.Dense(len(output_voc))(dec_output)\n",
        "  predictions = tf.nn.softmax(out_pred, axis=-1)\n",
        "\n",
        "  model = tf.keras.Model([input_token, output_token], predictions)\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "transformer = get_transformer_model(output_voc)\n",
        "out = transformer((input_seq, output_seq))\n",
        "print(out.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_98\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_149 (InputLayer)          [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_layer_172 (EmbeddingL (None, 5, 256)       1280        input_149[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_150 (InputLayer)          [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_26 (TFOpLa (None, 5, 256)       0           embedding_layer_172[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "embedding_layer_173 (EmbeddingL (None, 6, 256)       1536        input_150[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "encoder_35 (Encoder)            (None, 5, 256)       1976832     tf.__operators__.add_26[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_27 (TFOpLa (None, 6, 256)       0           embedding_layer_173[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "decoder_29 (Decoder)            (None, 6, 256)       1976832     encoder_35[0][0]                 \n",
            "                                                                 tf.__operators__.add_27[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 6, 8)         2056        decoder_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.softmax_3 (TFOpLambda)    (None, 6, 8)         0           dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,958,536\n",
            "Trainable params: 3,958,536\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "(1, 6, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
